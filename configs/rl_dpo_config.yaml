# Configuration for RL Training (DPO)

# Base model and SFT checkpoint
model:
  base_name: "Qwen/Qwen2-7B-Instruct"
  sft_checkpoint: "models/sft"

# DPO-specific parameters
dpo:
  beta: 0.1  # DPO temperature parameter
  max_length: 1024
  max_prompt_length: 512

# Training parameters
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  epochs: 3
  learning_rate: 1e-5
  seed: 42
  fp16: true
  gradient_checkpointing: true

# Instruction template
instruction:
  template: "Translate the following {src_lang} text to Chinese:\n\n{src_text}"

# Paths
paths:
  train_file: "data/rl/dpo_train.jsonl"
  valid_file: "data/rl/dpo_valid.jsonl"
  output_dir: "models/rl_dpo"

# Generation parameters for creating preference pairs
generation:
  num_candidates: 4
  temperature: 0.7
  top_p: 0.9
