# Configuration for SFT Training

# Base model
model:
  name: "Qwen/Qwen2-7B-Instruct"
  max_length: 1024

# LoRA configuration
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"

# QLoRA (4-bit quantization)
qlora:
  enabled: false  # Set to true for QLoRA

# Training parameters
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  epochs: 3
  learning_rate: 2e-4
  warmup_ratio: 0.03
  seed: 42
  fp16: true
  gradient_checkpointing: true

# Instruction template
instruction:
  template: "Translate the following {src_lang} text to Chinese:\n\n{src_text}"

# Paths
paths:
  train_file: "data/sft/train.jsonl"
  valid_file: "data/sft/valid.jsonl"
  output_dir: "models/sft"

# Generation parameters for evaluation
generation:
  max_new_tokens: 512
  num_beams: 1
  do_sample: false
  temperature: 1.0
