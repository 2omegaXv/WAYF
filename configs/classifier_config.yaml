# Configuration for Classifier Training

# Model configuration
model:
  # backbone: "hfl/chinese-roberta-wwm-ext"
  backbone: "models/hf_backbones/chinese-roberta-wwm-ext"
  hidden_dim: 512
  dropout: 0.1

# Training parameters
training:
  max_length: 512
  batch_size: 32
  epochs: 5
  learning_rate: 1e-4
  warmup_ratio: 0.1
  seed: 42

lora:
  r: 8
  alpha: 16
  dropout: 0.1

# Baseline models to train (optional)
baselines:
  - "majority"
  - "stratified_random"
  - "ngram_logistic"
  - "ngram_svm"
  - "fasttext"
  - "frozen_backbone"

# N-gram settings for baselines (optional)
ngram:
  range: [3, 6]
  max_features: 10000

experiment:
  translation_model: "DeepSeek-V3.2"
  pool: "a"

# Paths
paths:
  # train_file: # used when want to specify certain files to train on
  #   - "new_data/translated/DeepSeek-V3.2/a_chinese_zh.jsonl"
  # valid_file: 
  # test_file: 
  output_dir: "models/classifier/linear_classifier"
  # output_dir: "models/classifier_stageA" # Changed for two-stage training

# Metrics to track
metrics:
  - accuracy
  - macro_f1
  - auc

hf:
  cache_dir: null
  local_dir: models/hf_backbones/chinese-roberta-wwm-ext

# Weights & Biases (optional)
# Note: Prefer setting WANDB_API_KEY in the environment. If you do set it here, the training script will export it to the process env.
wandb:
  enabled: True
  api_key: 
  entity: null
  project: "WAYF"
  name: # run name
  tags: []
  notes: null
  mode: "online"   # online | offline | disabled
